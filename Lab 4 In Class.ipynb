{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 4: Mode Choice"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Estimation Dataset\n",
    "\n",
    "We are using the same 2017 survey trip records for estimation. However, the data has been pre-processed for you to allow for model estimation. The file **'trip17_estimation.csv'** contains all of the observed rows, but also has additional rows for the unchosen alternative by mode. To measure and estimate the relative value of one mode alternative to the others, we need data on those other alternatives, which generally aren't provided in a standard survey. \n",
    "\n",
    "This dataset was generated by duplicating all observed (chosen) trips (total length x) for each mode (m) alternative (once for walk alternatives, once for bike, etc.), to end up with a new dataset that was of length (x)(m). Existing PSRC scripts were used to add travel time, cost, and distance for these trip alternatives, using model outputs. Model outputs report time/cost/distance between all origins and destinations (often called 'skims'), so it's possible to provided measures for all trips, even those that are highly unlikely (like biking from Tacoma to Everett). Some modellers have used other means of attaching these \"skim\" values to alternatives, including Google APIs. There is usually a cost associated with this since API calls are capped, and model skims are readily available, so it was easier to use the old method. However, with more observed data in the world, there are plenty of alternatives becoming available!\n",
    "\n",
    "----\n",
    "\n",
    "## Data Dictionary\n",
    "### Modes:\n",
    "- 1: Walk\n",
    "- 2: Bike\n",
    "- 3: SOV (single occupant vehicle)\n",
    "- 4: HOV2 (2 people in a vehicle)\n",
    "- 5: HOV3 (3 or more people in a vehicle)\n",
    "- 6: Transit (including bus and train)\n",
    "- 9: TNC (hired rideshare vehicles like Uber, Lyft)\n",
    "\n",
    "### Sociodemographics and Land Use:\n",
    "Same as past codebook unless otherwise noted\n",
    "- choice: 0/1 for if record represents observed choice data\n",
    "- hh_density_o: households/ft^2 in trip's origin TAZ\n",
    "- emp_density_o: total employees/ft^2 in trip's origin TAZ\n",
    "- college_density_o: college students/ft^2 in trip's origin TAZ\n",
    "- gradeschool_density_o: gradeschool students/ft^2 in trip's origin TAZ\n",
    "- dist_lbus: distance to transit in miles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pylogit\n",
    "\n",
    "There are multiple libraries that can be used to estimate choice models in Python, including:\n",
    "- [statsmodels](https://www.statsmodels.org/stable/index.html)\n",
    "- [pylogit](https://github.com/timothyb0912/pylogit)\n",
    "- [choicemodels (built partially off pylogit)](https://github.com/UDST/choicemodels)\n",
    "- [biogeme](http://biogeme.epfl.ch/examples_swissmetro.html)\n",
    "\n",
    "We will focus on pylogit because it is easier and more flexible to use than statsmodels, and more developed than choicemodels, which is still in development. Biogeme is an old tool that is now available in pandas, which is very nice, but the documentation is somewhat lacking. You are welcome to try multiple libraries, but I will only be able to provide direct supply for pylogit.\n",
    "\n",
    "For more information, see: https://github.com/timothyb0912/pylogit/tree/master/examples/notebooks. \n",
    "- You can open HTML notebooks from these links, e.g. https://github.com/timothyb0912/pylogit/blob/master/examples/notebooks/Main%20PyLogit%20Example.ipynb\n",
    "- If interested in nested logit, see example: https://github.com/timothyb0912/pylogit/blob/master/examples/notebooks/Nested%20Logit%20Example--Python%20Biogeme%20benchmark--09NestedLogit.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Import the pylogit library\n",
    "import pylogit as pl    # Importing as shortcut \"pl\", similar to pandas imported as \"pd\"\n",
    "from collections import OrderedDict    # a requirement for pylogit specifications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a list of HBW trips in estimation format\n",
    "\n",
    "df_hbw = pd.read_csv(r'trip17_estimation_hbw.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "608"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# what does the trips dataset look like now?\n",
    "df_hbw.hhid.nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MNL Estimation Specification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We are using what is called an Ordered Dictionary\n",
    "# Remember that a dictionary looks like this {'key': 'value'}\n",
    "# An ordered dictionary is a special version of this type that keeps the keys in order\n",
    "\n",
    "specification = OrderedDict()\n",
    "names = OrderedDict()\n",
    "\n",
    "# Define the alternative specific constants (ASCs), i.e., the intercepts\n",
    "# Remember that one choice is the baseline (ASC=0)\n",
    "# Leave the chosen baseline mode out of the list of intercept values below\n",
    "# In this case, we are using SOV as the base model - it's common to use the most likely alternative as the base\n",
    "specification[\"intercept\"] = [1, 2, 4, 5, 6, 9]    # these are the mode IDs, excluding 3 for SOV\n",
    "names['intercept'] = ['Walk ASC','Bike ASC', 'HOV2 ASC','HOV3+ ASC','Transit ASC', 'TNC']    # Provide labels\n",
    "\n",
    "# Create a coefficient for travel time\n",
    "# Note that we are using only a single travel time coefficient across all modes\n",
    "\n",
    "specification['travtime'] = [[1,2,3,4,5,6,9]]    # Note that this is a list inside a list [[]]\n",
    "names['travtime'] = ['time all']\n",
    "\n",
    "# Specify which columns are used in the estimation\n",
    "custom_alt_id = \"mode_id\"    # Mode columns, must be integer based\n",
    "obs_id_column = \"custom_id\"    # an ID that is unique to each choice (a set of chosen and unchosen alternatives have their own ID)\n",
    "choice_column = \"choice\"    # 0/1 column indicating if that row was the chosen or unchosen alternative\n",
    "\n",
    "# Call the module to create the choice model specification\n",
    "model_1 = pl.create_choice_model(data=df_hbw,    # Note that here's we are specifying the df_hbw dataset\n",
    "                                alt_id_col=custom_alt_id,\n",
    "                                obs_id_col=obs_id_column,\n",
    "                                choice_col=choice_column,\n",
    "                                specification=specification,    # using the basic_specification from above\n",
    "                                model_type=\"MNL\",\n",
    "                                names=names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('intercept', [1, 2, 4, 5, 6, 9]),\n",
       "             ('travtime', [[1, 2, 3, 4, 5, 6, 9]])])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "specification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log-likelihood at zero: -9,595.2829\n",
      "Initial Log-likelihood: -9,595.2829\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kaitlynng/opt/anaconda3/lib/python3.9/site-packages/scipy/optimize/_minimize.py:527: RuntimeWarning: Method BFGS does not use Hessian information (hess).\n",
      "  warn('Method %s does not use Hessian information (hess).' % method,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimation Time for Point Estimation: 0.50 seconds.\n",
      "Final log-likelihood: -6,436.4076\n",
      "             parameters  std_err  t_stats  p_values  robust_std_err  \\\n",
      "Walk ASC          0.705    0.074    9.493       0.0           0.127   \n",
      "Bike ASC         -1.554    0.073  -21.240       0.0           0.084   \n",
      "HOV2 ASC         -1.735    0.054  -31.944       0.0           0.054   \n",
      "HOV3+ ASC        -3.433    0.119  -28.871       0.0           0.119   \n",
      "Transit ASC      -0.355    0.036   -9.839       0.0           0.039   \n",
      "TNC              -3.548    0.126  -28.204       0.0           0.126   \n",
      "time all         -0.038    0.002  -23.033       0.0           0.004   \n",
      "\n",
      "             robust_t_stats  robust_p_values  \n",
      "Walk ASC              5.565              0.0  \n",
      "Bike ASC            -18.402              0.0  \n",
      "HOV2 ASC            -31.942              0.0  \n",
      "HOV3+ ASC           -28.870              0.0  \n",
      "Transit ASC          -9.093              0.0  \n",
      "TNC                 -28.204              0.0  \n",
      "time all             -9.896              0.0  \n",
      "                     Multinomial Logit Model Regression Results                    \n",
      "===================================================================================\n",
      "Dep. Variable:                      choice   No. Observations:                4,933\n",
      "Model:             Multinomial Logit Model   Df Residuals:                    4,926\n",
      "Method:                                MLE   Df Model:                            7\n",
      "Date:                     Fri, 03 Nov 2023   Pseudo R-squ.:                   0.329\n",
      "Time:                             09:34:32   Pseudo R-bar-squ.:               0.328\n",
      "AIC:                            12,886.815   Log-Likelihood:             -6,436.408\n",
      "BIC:                            12,932.341   LL-Null:                    -9,595.283\n",
      "===============================================================================\n",
      "                  coef    std err          z      P>|z|      [0.025      0.975]\n",
      "-------------------------------------------------------------------------------\n",
      "Walk ASC        0.7054      0.074      9.493      0.000       0.560       0.851\n",
      "Bike ASC       -1.5545      0.073    -21.240      0.000      -1.698      -1.411\n",
      "HOV2 ASC       -1.7347      0.054    -31.944      0.000      -1.841      -1.628\n",
      "HOV3+ ASC      -3.4333      0.119    -28.871      0.000      -3.666      -3.200\n",
      "Transit ASC    -0.3546      0.036     -9.839      0.000      -0.425      -0.284\n",
      "TNC            -3.5483      0.126    -28.204      0.000      -3.795      -3.302\n",
      "time all       -0.0379      0.002    -23.033      0.000      -0.041      -0.035\n",
      "===============================================================================\n"
     ]
    }
   ],
   "source": [
    "# The code above only generated the template to estimate the model. We still need to execute the actual\n",
    "# maximum likelihood estimation process. \n",
    "\n",
    "# Run the estimation given the specification \"basic_mnl\"\n",
    "\n",
    "# Specify the initial values and method for the optimization.\n",
    "# Note that the value in np.zeros() is the number of coefficients we expect to return, including ASCs\n",
    "# For the basic setup above there are 6 ASC alternatives and 1 travel time variable, for a total of 7\n",
    "# Note that the error result if you get this number wrong will usually give the required value. \n",
    "\n",
    "# Note that here's were using a method called \"fit_mile\" on the object \"basic_mnl,\" which we created above. \n",
    "model_1.fit_mle(np.zeros(7))\n",
    "print(np.round(model_1.summary, 3))    # Make things easier to read\n",
    "\n",
    "# Look at the estimation results\n",
    "print(model_1.get_statsmodels_summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
